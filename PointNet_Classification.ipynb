{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPgXqGvZ6P4cQQYfDsb1nps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/esalagran/DeepLearningProject/blob/master/PointNet_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MmFozKMPk4x",
        "outputId": "3b6de2e8-cc2b-4d05-db96-98e8dc677135"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun 21 11:49:51 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNMWJ1XHSzXR",
        "outputId": "1bb4d79e-f10b-433d-93ed-ae239cebbcdf"
      },
      "source": [
        "!python -c \"import torch; print(torch.version.cuda)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nja0XjwDS6qe",
        "outputId": "8b6abe49-7e5e-4a6f-aa5c-e15aa1885626"
      },
      "source": [
        "!python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxL6xzCgB6j4"
      },
      "source": [
        "###**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzR9qPQfB9-y",
        "outputId": "2694904e-6478-4cae-a95f-696e727847ed"
      },
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-geometric -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "\n",
        "!pip install -q path\n",
        "!pip install torchmetrics\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "import numpy as np\n",
        "from path import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics import Accuracy, IoU\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.datasets import ShapeNet\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import datetime\n",
        "import itertools\n",
        "import tensorflow\n",
        "import tensorboard\n",
        "from time import time\n",
        "from tensorboard import notebook\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.7)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.10)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.7/dist-packages (1.5.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.9.0+cu102)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8Xv57PDBtZU"
      },
      "source": [
        "###**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIy0Rj3tBwYz"
      },
      "source": [
        "hparams = {\n",
        "    'k': 3,\n",
        "    'num_classes': 16,\n",
        "    'lr': 1e-3,\n",
        "    'bs': 32,\n",
        "    'num_workers': 2,\n",
        "    'shuffle_train': False,\n",
        "    'shuffle_valid': False,\n",
        "    'epochs': 10,\n",
        "    'schedule': False\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL5dRNFjCWdE"
      },
      "source": [
        "###**PointNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNll8lDaCWuy"
      },
      "source": [
        "class TNet(nn.Module):\n",
        "  def __init__(self, k=3):\n",
        "    super().__init__()\n",
        "    self.k = k\n",
        "    self.sharedMLP = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=k, out_channels=64, kernel_size=1, bias=True),\n",
        "        nn.BatchNorm1d(64), \n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1, bias=True),\n",
        "        nn.BatchNorm1d(128), \n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1, bias=True),\n",
        "        nn.BatchNorm1d(1024), \n",
        "        nn.ReLU()\n",
        "    )\n",
        "    self.FC = nn.Sequential(\n",
        "        nn.Linear(in_features=1024, out_features=512, bias=True),\n",
        "        nn.BatchNorm1d(512), \n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=512, out_features=256, bias=True),\n",
        "        nn.BatchNorm1d(256), \n",
        "        nn.ReLU(),\n",
        "        # nn.Dropout(p=0.7), only apply when overfitting\n",
        "        nn.Linear(in_features=256, out_features=k*k), \n",
        "    )\n",
        "\n",
        "  def forward(self, cloud_points):\n",
        "    bs = cloud_points.size(0)\n",
        "    x = self.sharedMLP(cloud_points)\n",
        "    # size: [batch size, 1024, # of points]\n",
        "    x = nn.MaxPool1d(x.size(-1))(x) # pool with kernel = # of points/batch\n",
        "    # size: [batch size, 1024, 1]\n",
        "    x = nn.Flatten(start_dim=1)(x) # flatten to get horizontal vector\n",
        "    # size: [batch size, 1024]\n",
        "    x = self.FC(x)\n",
        "    # diagonal matrices initialized, as many as batch size\n",
        "    init_matrix = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
        "    if torch.cuda.is_available() and x.is_cuda:\n",
        "      init_matrix = init_matrix.cuda() # gets updated according to f.c. output\n",
        "    matrix = x.view(-1, self.k, self.k) + init_matrix\n",
        "    return matrix\n",
        "\n",
        "class Transform(nn.Module):\n",
        "   def __init__(self, k=3):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        # Input transform + First Shared MLP:\n",
        "        self.input_transform = TNet(k)\n",
        "        self.fc1 = nn.Sequential(\n",
        "          nn.Conv1d(in_channels=3, out_channels=64, kernel_size=1, bias=True),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm1d(64),\n",
        "          #nn.Conv1d(in_channels=64, out_channels=64, kernel_size=1, bias=True),\n",
        "          #nn.ReLU(),\n",
        "          #nn.BatchNorm1d(64)\n",
        "        )\n",
        "        # Feature transform + Second Shared MLP: \n",
        "        self.feature_transform = TNet(k=64)\n",
        "        self.fc2 = nn.Sequential(\n",
        "          # it is not necessary\n",
        "          # nn.Conv1d(in_channels=64, out_channels=64, kernel_size=1, bias=True),\n",
        "          # nn.BatchNorm1d(64), \n",
        "          # nn.ReLU(),\n",
        "          nn.Conv1d(in_channels=64, out_channels=128, kernel_size=1, bias=True),\n",
        "          nn.BatchNorm1d(128), \n",
        "          nn.ReLU(),\n",
        "          nn.Conv1d(in_channels=128, out_channels=1024, kernel_size=1, bias=True),\n",
        "          nn.BatchNorm1d(1024), \n",
        "          #nn.ReLU()\n",
        "        )\n",
        "\n",
        "   def forward(self, x):\n",
        "        n_points = x.size()[2]\n",
        "        # ------------------------------------------------------------\n",
        "        matrix3x3 = self.input_transform(x)\n",
        "        x = torch.bmm(torch.transpose(x,1,2),matrix3x3).transpose(1,2)\n",
        "        x = self.fc1(x)\n",
        "        # ------------------------------------------------------------\n",
        "        matrix64x64 = self.feature_transform(x)\n",
        "        y = torch.bmm(torch.transpose(x,1,2), matrix64x64).transpose(1,2) \n",
        "        x = self.fc2(y)\n",
        "        # Maxpool \n",
        "        y = nn.MaxPool1d(x.size(-1))(x)\n",
        "        \"\"\"\n",
        "        x = nn.Flatten(1)(x).repeat(n_points,1,1).transpose(0,2).transpose(0,1)\n",
        "        y = torch.cat((x, y.repeat(1,1,n_points)), 1)\n",
        "        \"\"\"\n",
        "        # ES: Might be interesting to add\n",
        "        # global_features = x.view(bs,-1)\n",
        "        # return global_features\n",
        "        return y, matrix3x3, matrix64x64        \n",
        "\n",
        "class PointNetModel(nn.Module):\n",
        "    def __init__(self, k=3, num_classes=16):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        # Transform + Last MLP\n",
        "        self.transform = Transform()\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=1024, out_channels=512, kernel_size=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=512, out_channels=256, kernel_size=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=256, out_channels=num_classes, kernel_size=1),\n",
        "            #nn.BatchNorm1d(k),\n",
        "            #nn.ReLU()\n",
        "        )\n",
        "        \"\"\"\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=128, out_channels=num_classes, kernel_size=1)\n",
        "        )\n",
        "        \"\"\"\n",
        "    def forward(self, x):\n",
        "        x, matrix3x3, matrix128x128 = self.transform(x)\n",
        "        output = self.fc1(x)\n",
        "        return output, F.softmax(output,dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDmXBzwqDLq_"
      },
      "source": [
        "###**ShapeNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLedmTaqDSAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8020cdd-dda4-4f89-959e-518bfe72a7ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "sn_train = ShapeNet(root='/content/drive/MyDrive/Dataset_ShapeNet/',split=\"train\",include_normals=False)\n",
        "sn_valid = ShapeNet(root='/content/drive/MyDrive/Dataset_ShapeNet/',split=\"val\",include_normals=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "v7l0onQoy_Nt",
        "outputId": "e367a668-33d0-4da8-9906-7b91b572e9b2"
      },
      "source": [
        "def get_loader(data, \n",
        "               bs=1, \n",
        "               num_workers=2, \n",
        "               shuffle=False):\n",
        "  # Function to load ShapeNet split into a DataLoader object:\n",
        "  data_loader = DataLoader(data, \n",
        "                          batch_size=bs,\n",
        "                          shuffle=shuffle,\n",
        "                          num_workers=num_workers)\n",
        "  return data_loader\n",
        "\n",
        "device =  torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_loader = get_loader(sn_train)\n",
        "model = PointNetModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for i, data in enumerate(train_loader,0):\n",
        "  datos = data\n",
        "  targets = datos.category.to(device)\n",
        "  points = datos.pos.unsqueeze(2).to(device)\n",
        "  #targets = datos.category.to(device)\n",
        "  preds, probs = model(points)\n",
        "  print(preds.shape)\n",
        "  print(targets.shape)\n",
        "  #print(preds.squeeze(-1))\n",
        "  print(criterion(preds.squeeze(-1), targets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ad0417660001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPointNetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.60 GiB already allocated; 15.75 MiB free; 13.67 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyMOcUYCZ3RA"
      },
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "  targets = []\n",
        "  counts = torch.unique(data.batch, return_counts=True)[1]\n",
        "  for i, subdata in enumerate(data.category):\n",
        "    targets.append(subdata.repeat(counts[i]))\n",
        "  targets = torch.cat(targets)\n",
        "  print(targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RojHdeiKDYtB"
      },
      "source": [
        "###**Train Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TLh4v9FDhWt"
      },
      "source": [
        "def get_loader(data, \n",
        "               bs=32, \n",
        "               num_workers=2, \n",
        "               shuffle=False):\n",
        "  # Function to load ShapeNet split into a DataLoader object:\n",
        "  data_loader = DataLoader(data, \n",
        "                          batch_size=bs,\n",
        "                          shuffle=shuffle,\n",
        "                          num_workers=num_workers)\n",
        "  return data_loader\n",
        "\n",
        "def train_epoch(model, \n",
        "                train_loader, \n",
        "                optimizer, \n",
        "                device, \n",
        "                criterion, \n",
        "                accuracy):\n",
        "  # List for epoch loss:\n",
        "  epoch_train_loss = []\n",
        "  # Model in train mode:\n",
        "  model.train()\n",
        "  # Metrics stored information reset:\n",
        "  accuracy.reset()\n",
        "  #iou.reset()\n",
        "  # Batch loop for training:\n",
        "  for i, data in enumerate(train_loader,0):\n",
        "      # Data retrieval from each bath:   \n",
        "      points = data.pos.unsqueeze(2).to(device)\n",
        "      targets = data.category.to(device)\n",
        "      # Forward pass:\n",
        "      preds, probs = model(points)\n",
        "      # Loss calculation + Backpropagation pass\n",
        "      optimizer.zero_grad()\n",
        "      loss = criterion(preds.squeeze(-1), targets).to(device)\n",
        "      epoch_train_loss.append(loss.item()) # save loss to later represent\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # Batch metrics calculation:\n",
        "      accuracy.update(probs.squeeze(),targets)\n",
        "      #iou.update(probs.squeeze(), targets)\n",
        "  # Mean epoch metrics calculation:\n",
        "  mean_loss = np.mean(epoch_train_loss)\n",
        "  mean_acc = accuracy.compute().item()\n",
        "  #mean_iou = iou.compute().item()\n",
        "  # Print of all metrics:\n",
        "  print('Train loss: ', mean_loss, \"| Acc.: \", mean_acc)\n",
        "  return mean_loss, mean_acc\n",
        "\n",
        "def valid_epoch(model, \n",
        "                valid_loader, \n",
        "                scheduler,\n",
        "                schedule, \n",
        "                device,\n",
        "                criterion, \n",
        "                accuracy):\n",
        "  # List for epoch loss:\n",
        "  epoch_valid_loss = []\n",
        "  # Model in validation (evaluation) mode:\n",
        "  model.eval()\n",
        "  # Metrics stored information reset:\n",
        "  accuracy.reset()\n",
        "  #iou.reset()\n",
        "  # Batch loop for validation:\n",
        "  with torch.no_grad():\n",
        "    for i,data in enumerate(valid_loader,0):\n",
        "        # Data retrieval from each bath:   \n",
        "        points = data.pos.unsqueeze(2).to(device)\n",
        "        #targets = data.category.to(device)\n",
        "        # Forward pass:\n",
        "        preds, probs = model(points)\n",
        "        # Loss calculation + Backpropagation pass\n",
        "        loss = criterion(preds.squeeze(-1), targets.repeat(points.shape[0])).to(device)\n",
        "        epoch_valid_loss.append(loss.item())\n",
        "        # Batch metrics calculation:\n",
        "        accuracy.update(probs.squeeze(),targets)\n",
        "        #iou.update(probs.squeeze(),targets)\n",
        "  # Mean epoch metrics calculation:\n",
        "  mean_loss = np.mean(epoch_valid_loss)\n",
        "  mean_acc = accuracy.compute().item()\n",
        "  #mean_iou = iou.compute().item()\n",
        "  # Learnign rate adjustment with scheduler:\n",
        "  if schedule == True: scheduler.step(mean_loss)\n",
        "  # Print of all metrics:\n",
        "  print('Valid loss: ', mean_loss, \"| Acc.: \", mean_acc)\n",
        "  return mean_loss, mean_acc\n",
        "\n",
        "def fit(train_data, \n",
        "        valid_data,\n",
        "        k=3,\n",
        "        num_classes=16, \n",
        "        bs=128, \n",
        "        num_workers=2, \n",
        "        epochs=20, \n",
        "        lr=0.001,\n",
        "        schedule=False, \n",
        "        shuffle_train=True, \n",
        "        shuffle_valid=False, \n",
        "        model_filename=None):\n",
        "  # Data Loaders for train and validation:\n",
        "  train_loader = get_loader(data=train_data, \n",
        "                            bs=bs, \n",
        "                            num_workers=num_workers, \n",
        "                            shuffle=shuffle_train)\n",
        "  valid_loader = get_loader(data=valid_data, \n",
        "                            bs=bs, \n",
        "                            num_workers=num_workers, \n",
        "                            shuffle=shuffle_valid)\n",
        "  \n",
        "  # Features:\n",
        "  device =  torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "  model = PointNetModel(k=k, num_classes=num_classes).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  scheduler = ReduceLROnPlateau(optimizer, patience=5)\n",
        "  criterion = nn.CrossEntropyLoss().to(device)\n",
        "  # Metrics:\n",
        "  accuracy = Accuracy(average='micro', compute_on_step=False).to(device)\n",
        "  #iou = IoU(num_classes=num_classes, absent_score=1, compute_on_step=False).to(device)\n",
        "  # Tensorboard Block:\n",
        "    # Avoid tensorboard crashing when adding embeddings:\n",
        "  tensorflow.io.gfile = tensorboard.compat.tensorflow_stub.io.gfile \n",
        "  tensorboard_name = \"tensorboad\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.csv'\n",
        "  logdir = os.path.join(Path('/content/drive/MyDrive/Dataset_ShapeNet/'), tensorboard_name)\n",
        "  writer = SummaryWriter(log_dir=logdir) # writer for each epoch\n",
        "  #writer_best = SummaryWriter(log_dir=logdir) # writer for best IoU value\n",
        "  \n",
        "  # Lists for epoch metrics:\n",
        "  train_loss = []\n",
        "  valid_loss = []\n",
        "  train_macc = []\n",
        "  valid_macc = []\n",
        "  #train_miou = []  \n",
        "  #valid_miou = []\n",
        "  \n",
        "  # Initialize value for Acc (which is used to save the model of best performance)\n",
        "  min_macc = 0\n",
        "  # Start training:\n",
        "  print('Start training...')\n",
        "  for epoch in range(epochs):\n",
        "    print(\"Epoch: \", epoch)\n",
        "    # Training epoch:\n",
        "    epoch_train_loss, epoch_train_macc = train_epoch(model, \n",
        "                                                    train_loader, \n",
        "                                                    optimizer, \n",
        "                                                    device, \n",
        "                                                    criterion, \n",
        "                                                    accuracy)\n",
        "    # Validation epoch:\n",
        "    epoch_valid_loss, epoch_valid_macc= valid_epoch(model,\n",
        "                                                    valid_loader, \n",
        "                                                    scheduler, schedule, \n",
        "                                                    device, \n",
        "                                                    criterion, \n",
        "                                                    accuracy)\n",
        "    # Add epoch metrics to lists:\n",
        "    train_loss.append(epoch_train_loss)\n",
        "    valid_loss.append(epoch_valid_loss)\n",
        "    train_macc.append(epoch_train_macc)\n",
        "    valid_macc.append(epoch_valid_macc)\n",
        "    #train_miou.append(epoch_train_miou)\n",
        "    #valid_miou.append(epoch_valid_miou)\n",
        "    # Function to write metrics in selected tensorboard:\n",
        "    #def write_metric_tb(tb, label, metric, epoch):\n",
        "    #  tb.add_scalar(label, np.array(metric), epoch)\n",
        "      # Write training and validation loss in tensorboard\n",
        "    #write_metric_tb(writer, 'Train loss', epoch_train_loss, epoch)\n",
        "    #write_metric_tb(writer, 'Valid loss', epoch_valid_loss, epoch)\n",
        "      # Write mean IoU and accuracy in tensorboard\n",
        "    #write_metric_tb(writer, 'Train Accuracy', epoch_train_macc, epoch)\n",
        "    #write_metric_tb(writer, 'Valid Accuracy', epoch_valid_macc, epoch)\n",
        "    #write_metric_tb(writer, 'Train IoU', epoch_train_miou, epoch)\n",
        "    #write_metric_tb(writer, 'Valid IoU', epoch_valid_miou, epoch)\n",
        "      # Save model and write in tensorboard: best if condition for IoU is fullfilled:\n",
        "    if epoch_valid_macc > min_macc:\n",
        "      min_maccu = epoch_valid_macc\n",
        "      '''\n",
        "      # Write training and validation loss in tensorboard: best\n",
        "      write_best_metric_tb(writer_best, 'Train loss', epoch_train_loss, epoch)\n",
        "      write_best_metric_tb(writer_best, 'Valid loss', epoch_valid_loss, epoch)\n",
        "      # Write mean IoU and accuracy in tensorboard: best\n",
        "      write_best_metric_tb(writer_best, 'Train Accuracy', epoch_train_macc, epoch)\n",
        "      write_best_metric_tb(writer_best, 'Valid Accuracy', epoch_valid_macc, epoch)\n",
        "      write_best_metric_tb(writer_best, 'Train IoU', epoch_train_miou, epoch)\n",
        "      write_best_metric_tb(writer_best, 'Valid IoU', epoch_valid_miou, epoch)\n",
        "      '''\n",
        "      # Save model:\n",
        "      #filename = model_filename if model_filename is not None else 'model_'+datetime.datetime.now().strftime(\"%Y%m%d\")+'.pth'\n",
        "      #save_path = os.path.join(Path('/content/drive/MyDrive/Dataset_ShapeNet/'), filename)\n",
        "      #torch.save(model.state_dict(), save_path)\n",
        "      #print('Model with %d loss overwritten. Model with %d loss saved to %s' %(min_macc, epoch_valid_loss, save_path))\n",
        "    \n",
        "    print('---------------------------------------------------------------------------------------------------------')\n",
        "  \n",
        "  # Plot for Losses:  \n",
        "  fig = plt.figure()\n",
        "  epochs = len(train_loss)\n",
        "  plt.plot(range(epochs), train_loss, 'bo', label='Train loss', ms=3)\n",
        "  plt.plot(range(epochs), valid_loss, 'ro', label='Valid loss', ms=3)\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.legend()\n",
        "  # Plots for Accuracy and IoU: \n",
        "  def print_metrics(metric1, metric2, label1, label2):\n",
        "    fig, ax1 = plt.subplots()\n",
        "    ax2 = ax1.twinx()\n",
        "    ax1.plot(range(epochs), metric1, 'go', ms=3)\n",
        "    ax2.plot(range(epochs), metric2, 'yo', ms=3)\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel(str(label1), color='g')\n",
        "    ax2.set_ylabel(str(label2), color='y')\n",
        "  print_metrics(valid_macc, train_macc, 'Valid Point Accuracy', 'Train Point Accuracy')\n",
        "\n",
        "  %tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWBTZvyFuo4"
      },
      "source": [
        "###**Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tICFBjNrFx4M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "b5b1147d-df2a-413f-ffed-2241a3c94603"
      },
      "source": [
        "fit(sn_train, sn_valid, hparams['k'], hparams['num_classes'], hparams['bs'], hparams['num_workers'], hparams['epochs'], hparams['lr'], hparams['schedule'], hparams['shuffle_train'], hparams['shuffle_valid'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "Epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c1f34e8b3901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_classes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'schedule'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shuffle_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shuffle_valid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-a2a098894759>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(train_data, valid_data, k, num_classes, bs, num_workers, epochs, lr, schedule, shuffle_train, shuffle_valid, model_filename)\u001b[0m\n\u001b[1;32m    155\u001b[0m                                                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                                                     \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                                                     accuracy)\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;31m# Validation epoch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     epoch_valid_loss, epoch_valid_macc= valid_epoch(model,\n",
            "\u001b[0;32m<ipython-input-5-a2a098894759>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, device, criterion, accuracy)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0;31m# Loss calculation + Backpropagation pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# save loss to later represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 53.39 GiB (GPU 0; 14.76 GiB total capacity; 10.01 GiB already allocated; 3.60 GiB free; 10.09 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxsKE8cI10mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfc3e8d-3111-4682-d544-629b6e3279cd"
      },
      "source": [
        "1+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}